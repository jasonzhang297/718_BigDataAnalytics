{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLmKS5AHyPcb"
   },
   "source": [
    "##### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmrHfEEjfShT"
   },
   "source": [
    "# Question 0 (-2 If not answered)\n",
    "Please provide the following the data so we can verify your github information and ensure accurate grading:\n",
    "- Your Name: Yunhan Zhang\n",
    "- Your SU ID: 405379315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "ZGSucvIvDWn2",
    "nbgrader": {
     "checksum": "c51f80b694da894627ba37be28c86055",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professors: \n",
    "  - Willard Williamson <wewillia@syr.edu>\n",
    "  - Emory Creel <emcreel@g.syr.edu>\n",
    "- Faculty Assistants: \n",
    "  - Warren Justin Fernandes <wjfernan@syr.edu>\n",
    "  - Ruchita Hiteshkumar Harsora <\trharsora@g.syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- Google Colab is the official class runtime environment so you should test your code on Colab before submission.\n",
    "- Do not modify cells marked as grading cells or marked as do not modify.\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Runtime `$\\rightarrow$ Factory reset runtime followed by Runtime $\\rightarrow$ Run All.  All runtime errors will result in a minimum penalty of half off.\n",
    "- All plots shall include descriptive title and axis labels.  Plot legends shall be included where possible.  Unless stated otherwise, plots can be made using any Python plotting package.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- import * is not allowed because it is considered a very bad coding practice and in some cases can result in a significant delay (which slows down the grading process) in loading imports.  For example, the statement `from sympy import *` is not allowed.  You must import the specific packages that you need. \n",
    "- The graders reserve the right to deduct points for subjective things we see with your code.  For example, if we ask you to create a pandas data frame to display values from an investigation and you hard code the values, we will take points off for that.  This is only one of many different things we could find in reviewing your code.  In general, write your code like you are submitting it for a code peer review in industry.  \n",
    "- Level of effort is part of our subjective grading.  For example, in cases where we ask for a more open ended investigation, some students put in significant effort and some students do the minimum possible to meet requirements.  In these cases, we may take points off for students who did not put in much effort as compared to students who put in a lot of effort.  We feel that the students who did a better job deserve a better grade.  We reserve the right to invoke level of effort grading at any time.\n",
    "- Your notebook must run from start to finish without requiring manual input by the graders.  For example, do not mount your personal Google drive in your notebook as this will require graders to perform manual steps.  In short, your notebook should run from start to finish with no runtime errors and no need for graders to perform any manual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZzy57K4yPce"
   },
   "source": [
    "I was very disappointed with the linear regression model accuracy releted to the insurance data set in homework 3.  I'm sure you were disappointed too.  In this homework, we will revisit the insurance data set and try to improve prediction scores.  Specifically, we will use random forest, gradient boosting trees, and deep learning to see if we can improve upon the scores achieved in homework 3.  Part 1 of the assignment will explore random forest and GBT.  Part 2 of the assignment will use deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hZxH4JRN3Vcj"
   },
   "outputs": [],
   "source": [
    "# Grading Cell\n",
    "enable_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDW3ScIk2Q_H"
   },
   "source": [
    "The following cell is used to read the insurance data set into the colab environment.  Do not change or modify the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iJu6j3SbjU2N"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Do not change or modify this cell\n",
    "# Need to install pyspark\n",
    "# if pyspark is already installed, will print a message indicating pyspark already installed\n",
    "pip install pyspark &> /dev/null\n",
    "\n",
    "# Download the data files from github\n",
    "# If the data file does not exist in the colab environment\n",
    "data_file_1=insurance.csv\n",
    "\n",
    "if [[ ! -f ./${data_file_1} ]]; then \n",
    "   # download the data file from github and save it in this colab environment instance\n",
    "   wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/${data_file_1} &> /dev/null\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFz91GzJ4XFj"
   },
   "source": [
    "# Assignment Specific Instructions\n",
    "Your grade for grid search problems in this assignment will be determined in part on level of effort and your model performance results as compared to other students in the class.\n",
    "\n",
    "In this assignment, we will be comparing scores between random forest, gradient boosting trees, and deep learning.  You are required to correctly use train / test / validation data sets for model comparison as outlined in lecture.  Use train and test sets to train and score individual models during grid search.  Only use validation data to compare scores between models. You must name your data sets exactly `train`, `test`, and `validation` so that the graders know what data set is being used in each question.  I will be taking off 1 letter grade (10 points) for not following these instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2XNxzBB2aZU"
   },
   "source": [
    "# Question 1 (10 pts)\n",
    "- Read the insurance data file into a spark data frame named `medical_df`.  Drop any rows that contain NAN / Null values.  Check the schema and fix if needed.  Perform needed feature engineering using **only** a string indexer to get ready for training decision trees.  One hot encoding is not needed for random forest - do not use one hot encoding or any other transformations other than string indexing. \n",
    "- Split the data into variables named exactly train, test, and validation. Set the spark randomSplit seed argument to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FYtA8Bkw3Aq1"
   },
   "outputs": [],
   "source": [
    "# import data and drop Null values\n",
    "medical_df_withna = spark.read.csv(\"insurance.csv\", inferSchema = True, header = True)\n",
    "medical_df_raw = medical_df_withna.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sex_pipe that uses the StringIndexer to encode the gender data\n",
    "sex_pipe = feature.StringIndexer(inputCol='sex', handleInvalid='skip',outputCol=\"sex_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smoker_pipe that uses the StringIndexer to encode the smoker data\n",
    "smoker_pipe = feature.StringIndexer(inputCol='smoker', handleInvalid='skip',outputCol=\"smoker_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a region_pipe uses the StringIndexer to encode the region data\n",
    "region_pipe = feature.StringIndexer(inputCol='region', handleInvalid='skip',outputCol = \"region_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Pipeline(stages=[feature.VectorAssembler(inputCols=['age', 'children', 'bmi', 'sex_index', 'smoker_index', \n",
    "                                                               'region_index'], outputCol = 'features')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pipe = Pipeline(stages=[sex_pipe, smoker_pipe, region_pipe, features])\n",
    "fitted_fe_pipe = fe_pipe.fit(medical_df_raw)\n",
    "medical_df = fitted_fe_pipe.transform(medical_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = medical_df_raw.randomSplit([0.6, 0.3, 0.1], seed = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tnepQUryJHKm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- children: integer (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- charges: double (nullable = true)\n",
      " |-- sex_index: double (nullable = false)\n",
      " |-- smoker_index: double (nullable = false)\n",
      " |-- region_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+--------+------+---------+-----------+---------+------------+------------+--------------------+\n",
      "|age|   sex|   bmi|children|smoker|   region|    charges|sex_index|smoker_index|region_index|            features|\n",
      "+---+------+------+--------+------+---------+-----------+---------+------------+------------+--------------------+\n",
      "| 19|female|  27.9|       0|   yes|southwest|  16884.924|      1.0|         1.0|         2.0|[19.0,0.0,27.9,1....|\n",
      "| 18|  male| 33.77|       1|    no|southeast|  1725.5523|      0.0|         0.0|         0.0|[18.0,1.0,33.77,0...|\n",
      "| 28|  male|  33.0|       3|    no|southeast|   4449.462|      0.0|         0.0|         0.0|[28.0,3.0,33.0,0....|\n",
      "| 33|  male|22.705|       0|    no|northwest|21984.47061|      0.0|         0.0|         1.0|[33.0,0.0,22.705,...|\n",
      "| 32|  male| 28.88|       0|    no|northwest|  3866.8552|      0.0|         0.0|         1.0|[32.0,0.0,28.88,0...|\n",
      "| 31|female| 25.74|       0|    no|southeast|  3756.6216|      1.0|         0.0|         0.0|[31.0,0.0,25.74,1...|\n",
      "| 46|female| 33.44|       1|    no|southeast|  8240.5896|      1.0|         0.0|         0.0|[46.0,1.0,33.44,1...|\n",
      "| 37|female| 27.74|       3|    no|northwest|  7281.5056|      1.0|         0.0|         1.0|[37.0,3.0,27.74,1...|\n",
      "| 37|  male| 29.83|       2|    no|northeast|  6406.4107|      0.0|         0.0|         3.0|[37.0,2.0,29.83,0...|\n",
      "| 60|female| 25.84|       0|    no|northwest|28923.13692|      1.0|         0.0|         1.0|[60.0,0.0,25.84,1...|\n",
      "| 25|  male| 26.22|       0|    no|northeast|  2721.3208|      0.0|         0.0|         3.0|[25.0,0.0,26.22,0...|\n",
      "| 62|female| 26.29|       0|   yes|southeast| 27808.7251|      1.0|         1.0|         0.0|[62.0,0.0,26.29,1...|\n",
      "| 23|  male|  34.4|       0|    no|southwest|   1826.843|      0.0|         0.0|         2.0|[23.0,0.0,34.4,0....|\n",
      "| 56|female| 39.82|       0|    no|southeast| 11090.7178|      1.0|         0.0|         0.0|[56.0,0.0,39.82,1...|\n",
      "| 27|  male| 42.13|       0|   yes|southeast| 39611.7577|      0.0|         1.0|         0.0|[27.0,0.0,42.13,0...|\n",
      "| 19|  male|  24.6|       1|    no|southwest|   1837.237|      0.0|         0.0|         2.0|[19.0,1.0,24.6,0....|\n",
      "| 52|female| 30.78|       1|    no|northeast| 10797.3362|      1.0|         0.0|         3.0|[52.0,1.0,30.78,1...|\n",
      "| 23|  male|23.845|       0|    no|northeast| 2395.17155|      0.0|         0.0|         3.0|[23.0,0.0,23.845,...|\n",
      "| 56|  male|  40.3|       0|    no|southwest|  10602.385|      0.0|         0.0|         2.0|[56.0,0.0,40.3,0....|\n",
      "| 30|  male|  35.3|       0|   yes|southwest|  36837.467|      0.0|         1.0|         2.0|[30.0,0.0,35.3,0....|\n",
      "+---+------+------+--------+------+---------+-----------+---------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the schema\n",
    "medical_df.printSchema()\n",
    "#Print the shape\n",
    "# print('The shape of the dataframe is:', shape(medical_df))\n",
    "# print the head\n",
    "medical_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-N1EcFY3CG2"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er1V2eZ23Hrp"
   },
   "source": [
    "The following questions will create a random forest regressor model.  The goal is to see if we can improve upon the linear regression score from homework 3. You can find the spark documentation for the random forest regressor [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XwLgVI34uP6"
   },
   "source": [
    "# Question 2 (10 pts)\n",
    "Create and train a random forest regressor model using a grid search in the cell below.  Score your model using MSE.  Your grid search must be entirely encapsulated in the `if enable_grid_search` if statement.  The `enable_grid_search` Boolean is defined in a grading cell above.  You will disable the grid search before you submit by setting enable_grid_search to false.  Setting enable_grid_search to false should not result in a runtime error.  You will not receive full credit if any part of your grid search is outside of the if statement or if runtime errros result from setting the `enable_grid_search` variable to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol = \"charges\")\n",
    "pipeline = Pipeline(stages=[fe_pipe, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7gfVl99j47Sy"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "if enable_grid_search:\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \\\n",
    "    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \\\n",
    "    .build()\n",
    "    \n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator().setLabelCol(\"charges\"),\n",
    "                          numFolds=3)\n",
    "    \n",
    "    model = crossval.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "    mse = evaluator.evaluate(predictions)\n",
    "    print(\"The Best Mean Squared Error (MSE) on test data = %g\" % mse)\n",
    "    bestModel = model.bestModel.stages[1]\n",
    "    print(\"NumTrees in the best model\", bestModel.getNumTrees)\n",
    "    print(\"maxDepth in the best model\", bestModel.getOrDefault('maxDepth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Refer to https://www.silect.is/blog/random-forest-models-in-spark-ml/. This transformer (i.e. prediction generator) from out cross-validator by default applies the best performing pipeline. We can test our new model by making predictions on the hold out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVFHESWM544r"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQcJEwqw57e6"
   },
   "source": [
    "# Question 3 (10 pts)\n",
    "Create a pipeline named `best_rf_pipe` that hard codes the tuning parameters from the best model found by the grid search in question 2 above.  Train and test best_rf_pipe.  Score your model using validation data and the MSE scoring metric.  Save train and validation MSE scores in variables named rf_train_mse and rf_validation_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OM-nZSd775bT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on train data = 1.76135e+07\n"
     ]
    }
   ],
   "source": [
    "MaxDepth = 25\n",
    "NumTrees = 50\n",
    "\n",
    "best_rf = RandomForestRegressor().\\\n",
    "        setLabelCol('charges').\\\n",
    "        setFeaturesCol('features').\\\n",
    "        setMaxDepth(MaxDepth).\\\n",
    "        setNumTrees(NumTrees)\n",
    "\n",
    "best_rf_pipe = Pipeline(stages=[fe_pipe, best_rf])\n",
    "model_q3 = best_rf_pipe.fit(train)\n",
    "predictions_q3 = model_q3.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "rf_train_mse = evaluator.evaluate(predictions_q3)\n",
    "print(\"Mean Squared Error (MSE) on train data = %g\" % rf_train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on validation data = 2.55118e+07\n"
     ]
    }
   ],
   "source": [
    "predictions_q3_val = model_q3.transform(validation)\n",
    "evaluator_val = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "rf_validation_mse = evaluator_val.evaluate(predictions_q3_val)\n",
    "print(\"Mean Squared Error (MSE) on validation data = %g\" % rf_validation_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yg0yePyXu-27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_train_mse = 17613516.100674648\n",
      "rf_validation_mse = 25511794.8633418\n"
     ]
    }
   ],
   "source": [
    "# Grading cell do not modify\n",
    "print(\"rf_train_mse =\", rf_train_mse)\n",
    "print(\"rf_validation_mse =\", rf_validation_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6MAsAZ77vr"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feuCoidR8HqE"
   },
   "source": [
    "# Question 4 (10 pts)\n",
    "Use `best_rf_pipe` in question 3 for inference.  Create a pandas data frame named `rf_feature_importance` which contains 2 columns: `feature`, and `importance`.  Load the feature column with the feature name and the importance column with the feature importance score as determined by the random forest model. Sort the feature importances from high to low such that the most important feature is in the first row of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C6pdzDU58JN0"
   },
   "outputs": [],
   "source": [
    "best_rf_model = model_q3.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_importance = pd.DataFrame(list(zip(medical_df.columns[2:], best_rf_model.featureImportances.toArray())),\n",
    "            columns = ['feature', 'importance']).sort_values('importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VXD8OV-J9MSP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>region</td>\n",
       "      <td>0.010451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sex_index</td>\n",
       "      <td>0.026427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children</td>\n",
       "      <td>0.027338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bmi</td>\n",
       "      <td>0.135357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smoker</td>\n",
       "      <td>0.143534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>charges</td>\n",
       "      <td>0.656893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  importance\n",
       "3     region    0.010451\n",
       "5  sex_index    0.026427\n",
       "1   children    0.027338\n",
       "0        bmi    0.135357\n",
       "2     smoker    0.143534\n",
       "4    charges    0.656893"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grading cell - do not modify\n",
    "display(rf_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM7mpmBD86Dk"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESAy3UR6VcyW"
   },
   "source": [
    "# Question 5 (10 pts)\n",
    "Repeat question 2 but this time use a [GBT regressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html#pyspark.ml.regression.GBTRegressor).  Create and train a GBT regressor model using a grid search in the cell below.  Score your model using validation data and the MSE scoring metric.  Your grid search must be entirely encapsulated in the `if enable_grid_search` if statement.  The `enable_grid_search` Boolean is defined in a grading cell above.  You will disable the grid search before you submit by setting enable_grid_search to false.  Setting enable_grid_search to false should not result in a runtime error.  You will not receive full credit if any part of your grid search is outside of the if statement or if runtime errros result from setting the `enable_grid_search` variable to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol = \"charges\")\n",
    "gbt_pipeline = Pipeline(stages=[fe_pipe, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rl0pz14SWPDM"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "if enable_grid_search:\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \\\n",
    "    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \\\n",
    "    .build()\n",
    "    \n",
    "    gbt_crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator().setLabelCol(\"charges\"),\n",
    "                          numFolds=3)\n",
    "    \n",
    "    gbt_model = gbt_crossval.fit(train)\n",
    "    gbt_predictions = gbt_model.transform(test)\n",
    "    gbt_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "    gbt_mse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "    print(\"The Best Mean Squared Error (MSE) on test data = %g\" % gbt_mse)\n",
    "    gbt_bestpip = gbt_model.bestModel\n",
    "    gbt_bestModel = gbt_bestpip.stages[1]\n",
    "    print(\"Best Pipeline: \", gbt_bestModel)\n",
    "    print(\"maxDepth in the best model\", gbt_bestModel.getOrDefault('maxDepth'))\n",
    "    print(gbt_bestModel.getOrDefault('stepSize'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg7zznJYY9CL"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5_5YKxLWSpO"
   },
   "source": [
    "# Question 6 (10 pts Total)\n",
    "This is a repeat of question 3 but for GBT.  Create a pipeline named `best_gbt_pipe` that hard codes the tuning parameters from the best model found by the grid search in question 5 above.  Train and test best_gbt_pipe using MSE as the scoring metric. Clearly print the resulting train and test MSE for `best_gbt_pipe` so it's easy for the graders to see your resulting MSEs.  Save train and test MSE scores in variables named gbt_train_mse and gbt_validation_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ixj_BtU9Xxdj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on train data = 2.48386e+07\n"
     ]
    }
   ],
   "source": [
    "MaxDepth = 5\n",
    "StepSize = 0.1\n",
    "\n",
    "best_gbt = GBTRegressor().\\\n",
    "        setLabelCol('charges').\\\n",
    "        setFeaturesCol('features').\\\n",
    "        setMaxDepth(MaxDepth).\\\n",
    "        setStepSize(StepSize)\n",
    "\n",
    "best_gbt_pipe = Pipeline(stages=[fe_pipe, best_gbt])\n",
    "model_q6 = best_gbt_pipe.fit(train)\n",
    "predictions_q6 = model_q6.transform(test)\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "gbt_train_mse = gbt_evaluator.evaluate(predictions_q6)\n",
    "print(\"Mean Squared Error (MSE) on train data = %g\" % gbt_train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on validation data = 2.84579e+07\n"
     ]
    }
   ],
   "source": [
    "predictions_q6_val = model_q6.transform(validation)\n",
    "gbt_evaluator_val = RegressionEvaluator(\n",
    "        labelCol=\"charges\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "gbt_validation_mse = gbt_evaluator_val.evaluate(predictions_q6_val)\n",
    "print(\"Mean Squared Error (MSE) on validation data = %g\" % gbt_validation_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbt_train_mse = 24838572.975433294\n",
      "gbt_validation_mse = 28457932.60573362\n"
     ]
    }
   ],
   "source": [
    "# Grading cell do not modify\n",
    "print(\"gbt_train_mse =\", gbt_train_mse)\n",
    "print(\"gbt_validation_mse =\", gbt_validation_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQU2-FiwY8TL"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuRpV9erXylr"
   },
   "source": [
    "# Question 7 (10 pts)\n",
    "Create a pandas dataframe named `rf_gbt_mse_compare` which contains 3 columns: Model, Train MSE, and Validation MSE.  Load the Model column with \"RF\" or \"GBT\", the Train MSE column with the corresponding train MSE, and the Validation MSE column with the corresponding validation MSE scores from the random forest / gradient boosted tree scores.  Use rf_train_mse, rf_validation_mse, gbt_train_mse, and gbt_validation_mse variables to load the dataframe.  \n",
    "\n",
    "GBT models usually produce better scores than random forest.  I am not sure if that will be the case for this dataset but you will be graded in comparison to other students in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Azf4yLL0YjY6"
   },
   "outputs": [],
   "source": [
    "data = {'Model': ['RF', 'GBT'],\n",
    "        'Train MSE': [rf_train_mse,gbt_train_mse],\n",
    "        'Validation MSE': [rf_validation_mse, gbt_validation_mse]}\n",
    "rf_gbt_mse_compare = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s4j21P8BYsig"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>1.761352e+07</td>\n",
       "      <td>2.551179e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBT</td>\n",
       "      <td>2.483857e+07</td>\n",
       "      <td>2.845793e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model     Train MSE  Validation MSE\n",
       "0    RF  1.761352e+07    2.551179e+07\n",
       "1   GBT  2.483857e+07    2.845793e+07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grading Cell Do Not Modify\n",
    "display(rf_gbt_mse_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilMVkpyCYkeb"
   },
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a7l7Rae7637"
   },
   "source": [
    "# Question 8 (-5 pts if not performed)\n",
    "Set the `enable_grid_search` Boolean variable to False in the grading cell at the top of this notebook.  Perform a __Runtime -> Disconnect and Delter Runtime__, __Runtime -> Run all__ test to verify there are no runtime errors.  Leave the `enable_grid_search` variable set to False and turn in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN3BGbtbkU56"
   },
   "source": [
    "# Question 9 (0 pts - get ready for part 2)\n",
    "In part 2 of the assignment, we want to see how deep learning MSE scores compare to RF and GBT.  In the cell below, hard code variables which contain train and test scores for the models in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3svft76gksVo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_train_mse = 17613516.100674648\n",
      "rf_validation_mse = 25511794.8633418\n",
      "gbt_train_mse = 24838572.975433294\n",
      "gbt_validation_mse = 28457932.60573362\n"
     ]
    }
   ],
   "source": [
    "# Grading Cell - do not modify\n",
    "# print the train and test scores\n",
    "print(\"rf_train_mse =\", rf_train_mse)\n",
    "print(\"rf_validation_mse =\", rf_validation_mse)\n",
    "print(\"gbt_train_mse =\", gbt_train_mse)\n",
    "print(\"gbt_validation_mse =\", gbt_validation_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7aGOd9OFlHEJ"
   },
   "outputs": [],
   "source": [
    "# uncomment and hard code the following variables using output from above.  \n",
    "# You will copy this code for use in part 2 question 0\n",
    "# hc_rf_train_mse =\n",
    "# hc_rf_validation_mse =\n",
    "# hc_gbt_train_mse =\n",
    "# hc_gbt_validation_mse = "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
